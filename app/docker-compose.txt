version: '3.8'

services:
  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "3241:8080"  # This port is mapped but not required for Spark Master by default
    user: "root"  # Run as root user
  
  spark-worker:
    image: bitnami/spark:3.5.0
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    ports:
      - "3240:3240"  # This port is mapped but not required for Spark Worker by default
    user: "root"  # Run as root user

  jupyter:
    image: my-pyspark-notebook:3.5.0
    container_name: jupyter
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - NB_UID=1000  # Use the UID of jovyan or an appropriate user
      - NB_GID=100   # Use the GID of jovyan or an appropriate group
    volumes:
      - ./work:/home/jovyan/work  # Mounts your local directory to Jupyter
    ports:
      - "3442:8888"
    depends_on:
      - spark-master
      - spark-worker
      - mysql
    user: "root"  # Run as root user

  namenode:
    image: sequenceiq/hadoop-docker:2.7.1
    container_name: namenode
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - HADOOP_NAMENODE_ADDRESS=namenode:9000
    ports:
      - "50070:50070"  # Web UI for Namenode
      - "9000:9000"    # RPC port for Namenode
    volumes:
      - hdfs-namenode:/hadoop/dfs/name  # Store NameNode data in Jupyter's "data" folder
    depends_on:
      - jupyter
    user: "root"
    ## Run only the first time raising the namenode
    command: >
      /bin/bash -c "
        /usr/local/hadoop/bin/hdfs namenode -format -force &&
        /etc/bootstrap.sh -namenode &&
        /usr/local/hadoop/bin/hdfs dfs -mkdir -p /user/jovyan &&
        /usr/local/hadoop/bin/hdfs dfs -chown -R jovyan:supergroup /user/jovyan &&
        /usr/local/hadoop/sbin/start-dfs.sh &&
        /usr/local/hadoop/sbin/start-yarn.sh &&
        tail -f /usr/local/hadoop/logs/*namenode*.out"

  datanode:
    image: sequenceiq/hadoop-docker:2.7.1
    container_name: datanode
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - HADOOP_NAMENODE_ADDRESS=namenode:9000 
    ports:
      - "50075:50075"  # Web UI for Datanode
    volumes:
      - hdfs-datanode:/hadoop/dfs/data
    depends_on:
      - namenode
      - jupyter
    user: "root"
    command: >
      /bin/bash -c "
      echo 'fs.defaultFS=hdfs://$HADOOP_NAMENODE_ADDRESS' >> /usr/local/hadoop/etc/hadoop/core-site.xml &&
      while ! (echo > /dev/tcp/namenode/9000); do
        echo 'Waiting for NameNode...';
        sleep 5;
      done &&
      /usr/local/hadoop/sbin/hadoop-daemon.sh start datanode &&
      tail -f /usr/local/hadoop/logs/*datanode*.out"

  mysql:
    image: mysql:8.0
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: root  
      # MYSQL_DATABASE: mydatabase  # Default database
      MYSQL_USER: user
      MYSQL_PASSWORD: pass
    ports:
      - "3306:3306"  # MySQL default port
    volumes:
      - mysql-data:/var/lib/mysql  # Persist MySQL data
    networks:
      - default  # Ensure it's accessible to the other containers

volumes:
  hdfs-namenode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hdfs-namenode  # Local directory for persistent storage
  hdfs-datanode:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: ./hdfs-datanode  # Local directory for persistent storage
  mysql-data:  # Volume for MySQL data persistence
    driver: local
