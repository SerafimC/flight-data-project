start hadoop inside container
/usr/local/hadoop/sbin/start-dfs.sh

acess container
docker exec -it namenode bash

docker build -t my-pyspark-notebook:3.5.0 .


namenode commands
/usr/local/hadoop/bin/hdfs dfs -mkdir -p /user/jovyan 
/usr/local/hadoop/bin/hdfs dfs -chown -R jovyan:supergroup /user/jovyan 
ou
/usr/local/hadoop/bin/hdfs dfs -chown -R jovyan:supergroup hdfs://namenode:9000/user/jovyan

/usr/local/hadoop/sbin/start-dfs.sh

/usr/local/hadoop/bin/hdfs dfs -ls hdfs://namenode:9000/



docker exec -it airflow-webserver /bin/bash
airflow users create \
          --username admin \
          --firstname FIRST_NAME \
          --lastname LAST_NAME \
          --role Admin \
          --email admin@example.org


sed -i 's/^worker_timeout = .*/worker_timeout = 3000/' ./airflow.cfg